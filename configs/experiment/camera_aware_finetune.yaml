# @package _global_

# Camera-aware SMART finetuning experiment
# Finetunes cross-attention layers on pretrained SMART model

defaults:
  - override /data: camera_aware.yaml
  - override /model: camera_aware_smart.yaml
  - override /trainer: default.yaml
  - override /callbacks: default.yaml
  - override /logger: wandb.yaml

# experiment
experiment_name: "camera_aware_finetune"
tags: ["camera", "finetune", "cross_attention"]

# Trainer settings for finetuning
trainer:
  max_epochs: 50
  check_val_every_n_epoch: 5
  precision: 16
  gradient_clip_val: 1.0
  
# Model settings
model:
  model_config:
    # Finetune settings
    pretrained_path: "/workspace/logs/pre_bc_E31.ckpt"
    finetune:
      freeze_encoder: true
      lr_multiplier: 0.1
    
    # Camera settings
    camera_embed_dim: 32
    cross_attn_layers: [2]  # Add cross-attention at layer 2
    
    # Learning rate for finetuning
    lr: 5e-5
    lr_warmup_steps: 500
    lr_total_steps: 10000
    lr_min_ratio: 0.1
    
    # Validation settings
    val_open_loop: true
    val_closed_loop: true
    n_rollout_closed_val: 3
    n_vis_batch: 1

# Callbacks
callbacks:
  model_checkpoint:
    monitor: "val_wosac_minADE"
    mode: "min"
    save_top_k: 3
    save_last: true

# Logging
logger:
  wandb:
    project: "camera-aware-smart"
    name: "${experiment_name}"
    tags: ${tags}